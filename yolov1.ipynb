{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f62490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, S=7, B=2, C=11):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = os.path.join(self.label_dir, os.path.basename(image_path).replace('.png', '.txt'))\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.B * 5 + self.C), dtype=torch.float32)\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    cls, x, y, w, h = map(float, line.strip().split())\n",
    "                    i = min(int(self.S * y), self.S - 1)\n",
    "                    j = min(int(self.S * x), self.S - 1)\n",
    "                    x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "                    if label_matrix[i, j, 4] == 0: \n",
    "                        label_matrix[i, j, 4] = 1 \n",
    "                        label_matrix[i, j, 0:4] = torch.tensor([x_cell, y_cell, w, h])\n",
    "                        class_start_index = self.B * 5\n",
    "                        label_matrix[i, j, class_start_index + int(cls)] = 1\n",
    "\n",
    "        return image, label_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6452473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=11):\n",
    "        super(YOLOv1, self).__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(192, 128, kernel_size=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=1), nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 8 * 8, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(4096, S * S * (C + B * 5))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x.view(-1, self.S, self.S, self.B * 5 + self.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd7b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=11, lambda_coord=5, lambda_noobj=0.5):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction='sum')\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        N = predictions.shape[0]\n",
    "        obj_mask = target[..., 4] > 0\n",
    "        noobj_mask = target[..., 4] == 0\n",
    "\n",
    "        coord_loss = 0\n",
    "        obj_conf_loss = 0\n",
    "        noobj_conf_loss = 0 \n",
    "        class_loss = 0\n",
    "\n",
    "        for b in range(N):\n",
    "            for i in range(self.S):\n",
    "                for j in range(self.S):=\n",
    "                    # OBJECT LOSS \n",
    "                    if obj_mask[b, i, j]:\n",
    "                        iou1 = iou(predictions[b, i, j, 0:4].unsqueeze(0), target[b, i, j, 0:4].unsqueeze(0))\n",
    "                        iou2 = iou(predictions[b, i, j, 5:9].unsqueeze(0), target[b, i, j, 0:4].unsqueeze(0))\n",
    "\n",
    "                        if iou1 >= iou2:\n",
    "                            responsible_box_preds = predictions[b, i, j, 0:5]\n",
    "                            other_box_conf_pred = predictions[b, i, j, 9] \n",
    "                        else:\n",
    "                            responsible_box_preds = predictions[b, i, j, 5:10]\n",
    "                            other_box_conf_pred = predictions[b, i, j, 4] \n",
    "                        \n",
    "                        gt_box = target[b, i, j, 0:5]\n",
    "\n",
    "                        # Coordinate loss \n",
    "                        responsible_box_preds_wh_sqrt = torch.sign(responsible_box_preds[2:4]) * torch.sqrt(torch.abs(responsible_box_preds[2:4] + 1e-6))\n",
    "                        gt_box_wh_sqrt = torch.sqrt(gt_box[2:4])\n",
    "                        coord_loss += self.lambda_coord * (self.mse(responsible_box_preds[:2], gt_box[:2]) + self.mse(responsible_box_preds_wh_sqrt, gt_box_wh_sqrt))\n",
    "                        \n",
    "                        # Object confidence loss \n",
    "                        obj_conf_loss += self.mse(responsible_box_preds[4:5], torch.ones_like(responsible_box_preds[4:5]))\n",
    "\n",
    "                        # No-object confidence loss \n",
    "                        noobj_conf_loss += self.lambda_noobj * self.mse(other_box_conf_pred.unsqueeze(0), torch.zeros_like(other_box_conf_pred.unsqueeze(0)))\n",
    "                        \n",
    "                        # Classification loss\n",
    "                        class_loss += self.mse(predictions[b, i, j, self.B*5:], target[b, i, j, self.B*5:])\n",
    "\n",
    "        noobj_conf_preds = predictions[noobj_mask][..., [4, 9]]\n",
    "        noobj_conf_targets = torch.zeros_like(noobj_conf_preds)\n",
    "        noobj_conf_loss += self.lambda_noobj * self.mse(noobj_conf_preds, noobj_conf_targets)\n",
    "\n",
    "        total_loss = (coord_loss + obj_conf_loss + noobj_conf_loss + class_loss) / N\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d70aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(boxes1, boxes2):\n",
    "    # Convert from (x, y, w, h) to (x1, y1, x2, y2)\n",
    "    boxes1_x1 = boxes1[..., 0] - boxes1[..., 2] / 2\n",
    "    boxes1_y1 = boxes1[..., 1] - boxes1[..., 3] / 2\n",
    "    boxes1_x2 = boxes1[..., 0] + boxes1[..., 2] / 2\n",
    "    boxes1_y2 = boxes1[..., 1] + boxes1[..., 3] / 2\n",
    "\n",
    "    boxes2_x1 = boxes2[..., 0] - boxes2[..., 2] / 2\n",
    "    boxes2_y1 = boxes2[..., 1] - boxes2[..., 3] / 2\n",
    "    boxes2_x2 = boxes2[..., 0] + boxes2[..., 2] / 2\n",
    "    boxes2_y2 = boxes2[..., 1] + boxes2[..., 3] / 2\n",
    "\n",
    "    inter_x1 = torch.max(boxes1_x1, boxes2_x1)\n",
    "    inter_y1 = torch.max(boxes1_y1, boxes2_y1)\n",
    "    inter_x2 = torch.min(boxes1_x2, boxes2_x2)\n",
    "    inter_y2 = torch.min(boxes1_y2, boxes2_y2)\n",
    "\n",
    "    inter_area = (inter_x2 - inter_x1).clamp(0) * (inter_y2 - inter_y1).clamp(0)\n",
    "\n",
    "    area1 = (boxes1_x2 - boxes1_x1) * (boxes1_y2 - boxes1_y1)\n",
    "    area2 = (boxes2_x2 - boxes2_x1) * (boxes2_y2 - boxes2_y1)\n",
    "\n",
    "    union = area1 + area2 - inter_area + 1e-6\n",
    "    return inter_area / union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b520abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, iou_thresh=0.5):\n",
    "    boxes = sorted(boxes, key=lambda x: x[1], reverse=True)\n",
    "    keep = []\n",
    "    while boxes:\n",
    "        best = boxes.pop(0)\n",
    "        keep.append(best)\n",
    "        boxes = [b for b in boxes if iou(\n",
    "            torch.tensor(b[2:]), torch.tensor(best[2:])) < iou_thresh]\n",
    "    return keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b653e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(preds, S=7, B=2, C=11, conf_thresh=0.1):\n",
    "    batch_size = preds.shape[0]\n",
    "    decoded = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        boxes = []\n",
    "        pred = preds[b]\n",
    "        for i in range(S):\n",
    "            for j in range(S):\n",
    "                for b_idx in range(B):\n",
    "                    offset = b_idx * 5\n",
    "                    conf = pred[i, j, offset + 4]\n",
    "                    if conf > conf_thresh:\n",
    "                        x = (pred[i, j, offset + 0] + j) / S\n",
    "                        y = (pred[i, j, offset + 1] + i) / S\n",
    "                        w = pred[i, j, offset + 2]\n",
    "                        h = pred[i, j, offset + 3]\n",
    "                        cls = torch.argmax(pred[i, j, B * 5:]).item()\n",
    "                        boxes.append([cls, conf.item(), x.item(), y.item(), w.item(), h.item()])\n",
    "        decoded.append(nms(boxes))\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device, class_names, image_dir, label_dir):\n",
    "    model.eval()\n",
    "    all_preds = {}\n",
    "    start = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, _) in enumerate(tqdm(loader, desc='Evaluating')):\n",
    "            imgs = imgs.to(device)\n",
    "            preds = model(imgs).cpu()\n",
    "            decoded = decode_predictions(preds)\n",
    "            \n",
    "            for j in range(imgs.size(0)):\n",
    "                global_idx = i * loader.batch_size + j\n",
    "                all_preds[global_idx] = decoded[j]\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"\\nInference Speed: {len(loader.dataset)/(end-start):.2f} FPS\")\n",
    "\n",
    "    coco_images, coco_annotations, ann_id = [], [], 1\n",
    "    img_files = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n",
    "    \n",
    "    for img_id, img_path in enumerate(img_files):\n",
    "        file_name = os.path.basename(img_path)\n",
    "        coco_images.append({\n",
    "            \"id\": img_id,\n",
    "            \"file_name\": file_name,\n",
    "            \"width\": 128,\n",
    "            \"height\": 128\n",
    "        })\n",
    "\n",
    "        label_path = os.path.join(label_dir, file_name.replace('.png', '.txt'))\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    cls, x, y, w, h = map(float, line.strip().split())\n",
    "                    x_min = (x - w / 2) * 128\n",
    "                    y_min = (y - h / 2) * 128\n",
    "                    coco_annotations.append({\n",
    "                        \"id\": ann_id,\n",
    "                        \"image_id\": img_id,\n",
    "                        \"category_id\": int(cls),\n",
    "                        \"bbox\": [x_min, y_min, w * 128, h * 128],\n",
    "                        \"area\": w * 128 * h * 128,\n",
    "                        \"iscrowd\": 0\n",
    "                    })\n",
    "                    ann_id += 1\n",
    "\n",
    "\n",
    "    categories = [{\"id\": i, \"name\": name, \"supercategory\": \"none\"} for i, name in enumerate(class_names)]\n",
    "\n",
    "    gt_json = {\n",
    "        \"info\": {\n",
    "            \"description\": \"Radar Dataset\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"year\": 2025,\n",
    "            \"contributor\": \"YourName\",\n",
    "            \"date_created\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        },\n",
    "        \"images\": coco_images,\n",
    "        \"annotations\": coco_annotations,\n",
    "        \"categories\": categories\n",
    "    }\n",
    "\n",
    "    with open(\"ground_truth.json\", 'w') as f:\n",
    "        json.dump(gt_json, f, indent=2)\n",
    "\n",
    "    pred_json = []\n",
    "    for img_id, preds in all_preds.items():\n",
    "        for pred in preds:\n",
    "            cls, conf, x, y, w, h = pred\n",
    "            x_min = (x - w / 2) * 128\n",
    "            y_min = (y - h / 2) * 128\n",
    "            pred_json.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": int(cls),\n",
    "                \"bbox\": [x_min, y_min, w * 128, h * 128],\n",
    "                \"score\": float(conf)\n",
    "            })\n",
    "\n",
    "    with open(\"predictions.json\", 'w') as f:\n",
    "        json.dump(pred_json, f, indent=2)\n",
    "\n",
    "    if not pred_json:\n",
    "        print(\"No predictions were made. Skipping mAP evaluation.\")\n",
    "        return {\"mAP\": 0.0, \"mAP50\": 0.0, \"mAP75\": 0.0}\n",
    "\n",
    "    coco_gt = COCO(\"ground_truth.json\")\n",
    "    coco_dt = coco_gt.loadRes(\"predictions.json\")\n",
    "    \n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    metrics = {\n",
    "        \"mAP\": coco_eval.stats[0],   # mAP @ IoU=0.5:0.95\n",
    "        \"mAP50\": coco_eval.stats[1],  # mAP @ IoU=0.50\n",
    "        \"mAP75\": coco_eval.stats[2]   # mAP @ IoU=0.75\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bef03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, device, dataset, num_images=5):\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        img, label = dataset[i] \n",
    "        gt_boxes = []\n",
    "        for row in range(dataset.S):\n",
    "            for col in range(dataset.S):\n",
    "                if label[row, col, 4] > 0: \n",
    "                    gx = (label[row, col, 0] + col) / dataset.S\n",
    "                    gy = (label[row, col, 1] + row) / dataset.S\n",
    "                    gw = label[row, col, 2]\n",
    "                    gh = label[row, col, 3]\n",
    "                    gt_boxes.append([gx, gy, gw, gh])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_tensor = img.unsqueeze(0).to(device)\n",
    "            preds = model(img_tensor).cpu()\n",
    "            decoded_preds = decode_predictions(preds, conf_thresh=0.2) \n",
    "\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.imshow(img.squeeze(0), cmap='gray')\n",
    "\n",
    "        for box in gt_boxes:\n",
    "            x, y, w, h = box\n",
    "            x_min = (x - w / 2) * 128\n",
    "            y_min = (y - h / 2) * 128\n",
    "            width, height = w * 128, h * 128\n",
    "            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='g', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        if decoded_preds[0]:\n",
    "            for box in decoded_preds[0]:\n",
    "                cls, conf, x, y, w, h = box\n",
    "                x_min = (x - w / 2) * 128\n",
    "                y_min = (y - h / 2) * 128\n",
    "                width, height = w * 128, h * 128\n",
    "                rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                plt.text(x_min, y_min - 5, f'Cls {cls}, C: {conf:.2f}', color='red', fontsize=8)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7625b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    class_names = [f\"Class{i}\" for i in range(11)]\n",
    "\n",
    "    image_dir = os.path.expanduser(\"~/myenv/data/RadDet40k128HW009Tv2/images/test\")\n",
    "    label_dir = os.path.expanduser(\"~/myenv/data/RadDet40k128HW009Tv2/labels/test\")\n",
    "\n",
    "    # Verify paths exist\n",
    "    assert os.path.isdir(image_dir), f\"Image directory not found: {image_dir}\"\n",
    "    assert os.path.isdir(label_dir), f\"Label directory not found: {label_dir}\"\n",
    "\n",
    "    print(f\"Found {len(os.listdir(image_dir))} images.\")\n",
    "    print(f\"Found {len(os.listdir(label_dir))} labels.\")\n",
    "\n",
    "    dataset = RadarDataset(image_dir, label_dir, C=len(class_names))\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = YOLOv1(C=len(class_names)).to(device)\n",
    "    criterion = YoloLoss(C=len(class_names))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        loop = tqdm(loader, desc=f\"Epoch {epoch+1}\")\n",
    "        total_loss = 0\n",
    "\n",
    "        for imgs, labels in loop:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1} avg loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "    metrics = evaluate(model, loader, device, class_names, image_dir, label_dir)\n",
    "    print(f\"\\nmAP: {metrics['mAP']:.4f}, mAP50: {metrics['mAP50']:.4f}, mAP75: {metrics['mAP75']:.4f}\")\n",
    "    print(\"Visualizing some predictions...\")\n",
    "    visualize_predictions(model, device, dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
