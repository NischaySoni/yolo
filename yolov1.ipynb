{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52e55e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, S=7, B=2, C=20):\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        image_paths = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n",
    "        label_paths = sorted(glob.glob(os.path.join(label_dir, '*.txt')))\n",
    "\n",
    "        image_map = {os.path.splitext(os.path.basename(p))[0]: p for p in image_paths}\n",
    "        label_map = {os.path.splitext(os.path.basename(p))[0]: p for p in label_paths}\n",
    "        common_keys = sorted(set(image_map.keys()) & set(label_map.keys()))\n",
    "\n",
    "        self.image_paths = [image_map[k] for k in common_keys]\n",
    "        self.label_paths = [label_map[k] for k in common_keys]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"L\")\n",
    "        img = self.transform(img)\n",
    "\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.B * 5 + self.C))\n",
    "\n",
    "        with open(self.label_paths[idx], 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                cls, x, y, w, h = map(float, line.strip().split())\n",
    "                i = min(int(self.S * y), self.S - 1)\n",
    "                j = min(int(self.S * x), self.S - 1)\n",
    "                x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "                if label_matrix[i, j, 4] == 0:\n",
    "                    label_matrix[i, j, 0:5] = torch.tensor([x_cell, y_cell, w, h, 1])\n",
    "                    label_matrix[i, j, 5 + int(cls)] = 1\n",
    "\n",
    "        return img, label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f079a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YOLOv1, self).__init__()\n",
    "        self.S, self.B, self.C = S, B, C\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(192, 128, kernel_size=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=1), nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 8 * 8, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, S * S * (C + B * 5))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x.view(-1, self.S, self.S, self.B * 5 + self.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=20, lambda_coord=5, lambda_noobj=0.5):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction='sum')\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        obj_mask = target[..., 4] == 1\n",
    "        noobj_mask = target[..., 4] == 0\n",
    "\n",
    "        # Localization loss\n",
    "        box_pred = predictions[obj_mask][..., 0:4]\n",
    "        box_target = target[obj_mask][..., 0:4]\n",
    "        coord_loss = self.lambda_coord * self.mse(box_pred, box_target)\n",
    "\n",
    "        # Object confidence loss\n",
    "        conf_pred = predictions[obj_mask][..., 4]\n",
    "        conf_target = target[obj_mask][..., 4]\n",
    "        obj_conf_loss = self.mse(conf_pred, conf_target)\n",
    "\n",
    "        # No-object confidence loss\n",
    "        noobj_conf_pred = predictions[noobj_mask][..., 4]\n",
    "        noobj_conf_target = target[noobj_mask][..., 4]\n",
    "        noobj_conf_loss = self.lambda_noobj * self.mse(noobj_conf_pred, noobj_conf_target)\n",
    "\n",
    "        # Classification loss\n",
    "        class_pred = predictions[obj_mask][..., 5:]\n",
    "        class_target = target[obj_mask][..., 5:]\n",
    "        class_loss = self.mse(class_pred, class_target)\n",
    "\n",
    "        return coord_loss + obj_conf_loss + noobj_conf_loss + class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(preds, S=7, B=2, C=20, conf_thresh=0.2):\n",
    "    batch_size = preds.shape[0]\n",
    "    decoded = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        pred = preds[b]\n",
    "        boxes = []\n",
    "\n",
    "        for i in range(S):\n",
    "            for j in range(S):\n",
    "                cell = pred[i, j]\n",
    "                for b_idx in range(B):\n",
    "                    offset = b_idx * 5\n",
    "                    conf = cell[offset + 4]\n",
    "                    if conf > conf_thresh:\n",
    "                        x = (cell[offset + 0] + j) / S\n",
    "                        y = (cell[offset + 1] + i) / S\n",
    "                        w = cell[offset + 2]\n",
    "                        h = cell[offset + 3]\n",
    "                        class_id = torch.argmax(cell[5 + B * 5:]).item()\n",
    "                        boxes.append([class_id, conf.item(), x, y, w, h])\n",
    "        decoded.append(boxes)\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def yolo_to_coco_annotations(image_dir, label_dir, class_names):\n",
    "    image_files = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n",
    "    coco_images = []\n",
    "    coco_annotations = []\n",
    "    ann_id = 1\n",
    "\n",
    "    for img_id, img_path in enumerate(image_files):\n",
    "        file_name = os.path.basename(img_path)\n",
    "        image = Image.open(img_path)\n",
    "        width, height = image.size\n",
    "\n",
    "        coco_images.append({\n",
    "            'id': img_id,\n",
    "            'file_name': file_name,\n",
    "            'width': width,\n",
    "            'height': height\n",
    "        })\n",
    "\n",
    "        label_path = os.path.join(label_dir, file_name.replace('.png', '.txt'))\n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                cls, x, y, w, h = map(float, line.strip().split())\n",
    "                x_min = (x - w / 2) * width\n",
    "                y_min = (y - h / 2) * height\n",
    "                box_w = w * width\n",
    "                box_h = h * height\n",
    "\n",
    "                coco_annotations.append({\n",
    "                    'id': ann_id,\n",
    "                    'image_id': img_id,\n",
    "                    'category_id': int(cls),\n",
    "                    'bbox': [x_min, y_min, box_w, box_h],\n",
    "                    'area': box_w * box_h,\n",
    "                    'iscrowd': 0\n",
    "                })\n",
    "                ann_id += 1\n",
    "\n",
    "    categories = [{'id': i, 'name': name} for i, name in enumerate(class_names)]\n",
    "    return {\n",
    "        'images': coco_images,\n",
    "        'annotations': coco_annotations,\n",
    "        'categories': categories\n",
    "    }\n",
    "\n",
    "def predictions_to_coco(preds_dict, image_dir):\n",
    "    image_files = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n",
    "    results = []\n",
    "    width, height = 128, 128\n",
    "\n",
    "    for img_id, file_name in enumerate(image_files):\n",
    "        preds = preds_dict.get(img_id, [])\n",
    "        for pred in preds:\n",
    "            cls, conf, x, y, w, h = pred\n",
    "            x_min = (x - w / 2) * width\n",
    "            y_min = (y - h / 2) * height\n",
    "            box_w = w * width\n",
    "            box_h = h * height\n",
    "\n",
    "            results.append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': int(cls),\n",
    "                'bbox': [x_min, y_min, box_w, box_h],\n",
    "                'score': float(conf)\n",
    "            })\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device, class_names, image_dir, label_dir):\n",
    "    model.eval()\n",
    "    all_preds = {}\n",
    "    start = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, _) in enumerate(tqdm(loader, desc='Evaluating')):\n",
    "            imgs = imgs.to(device)\n",
    "            preds = model(imgs).cpu()\n",
    "            decoded = decode_predictions(preds)\n",
    "            for j in range(imgs.size(0)):\n",
    "                global_idx = i * loader.batch_size + j\n",
    "                all_preds[global_idx] = decoded[j]\n",
    "\n",
    "    end = time.time()\n",
    "    fps = len(loader.dataset) / (end - start)\n",
    "    print(f\"\\n✅ Inference Speed: {fps:.2f} FPS\")\n",
    "\n",
    "    # Ground Truth JSON (COCO-style) \n",
    "    gt_json = {\n",
    "        \"info\": {\n",
    "            \"description\": \"RadDet COCO-style ground truth\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"year\": 2025,\n",
    "            \"contributor\": \"YourName\",\n",
    "            \"date_created\": \"2025-07-11\"\n",
    "        },\n",
    "        \"licenses\": [],\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": i, \"name\": name} for i, name in enumerate(class_names)]\n",
    "    }\n",
    "\n",
    "    ann_id = 1\n",
    "    image_id = 1\n",
    "\n",
    "    for filename in sorted(os.listdir(label_dir)):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "        img_filename = filename.replace('.txt', '.png')\n",
    "        img_path = os.path.join(image_dir, img_filename)\n",
    "\n",
    "        # Safely get image size\n",
    "        with Image.open(img_path) as im:\n",
    "            width, height = im.size\n",
    "\n",
    "        gt_json[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": img_filename,\n",
    "            \"width\": width,\n",
    "            \"height\": height\n",
    "        })\n",
    "\n",
    "        with open(os.path.join(label_dir, filename), 'r') as f:\n",
    "            for line in f:\n",
    "                cls, x, y, w, h = map(float, line.strip().split())\n",
    "                bbox = [\n",
    "                    (x - w / 2) * width,    # x_min\n",
    "                    (y - h / 2) * height,   # y_min\n",
    "                    w * width,              # width\n",
    "                    h * height              # height\n",
    "                ]\n",
    "                gt_json[\"annotations\"].append({\n",
    "                    \"id\": ann_id,\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": int(cls),\n",
    "                    \"bbox\": bbox,\n",
    "                    \"area\": bbox[2] * bbox[3],\n",
    "                    \"iscrowd\": 0\n",
    "                })\n",
    "                ann_id += 1\n",
    "\n",
    "        image_id += 1\n",
    "\n",
    "    with open('ground_truth.json', 'w') as f:\n",
    "        json.dump(gt_json, f)\n",
    "\n",
    "    # Predictions JSON (COCO-style) \n",
    "    pred_json = []\n",
    "    image_id = 1\n",
    "\n",
    "    for idx, predictions in all_preds.items():\n",
    "        img_filename = sorted(os.listdir(image_dir))[idx]\n",
    "        with Image.open(os.path.join(image_dir, img_filename)) as im:\n",
    "            width, height = im.size\n",
    "\n",
    "        for pred in predictions:\n",
    "            cls, conf, x, y, w, h = pred\n",
    "            bbox = [\n",
    "                (x - w / 2) * width,\n",
    "                (y - h / 2) * height,\n",
    "                w * width,\n",
    "                h * height\n",
    "            ]\n",
    "            pred_json.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": int(cls),\n",
    "                \"bbox\": bbox,\n",
    "                \"score\": float(conf)\n",
    "            })\n",
    "\n",
    "        image_id += 1\n",
    "\n",
    "    with open('predictions.json', 'w') as f:\n",
    "        json.dump(pred_json, f)\n",
    "\n",
    "    if len(pred_json) == 0:\n",
    "        print(\"⚠️ No predictions above threshold. Skipping mAP evaluation.\")\n",
    "        return\n",
    "\n",
    "    # Run COCO Evaluation \n",
    "    coco_gt = COCO('ground_truth.json')\n",
    "    coco_dt = coco_gt.loadRes('predictions.json')\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b7485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    class_names = ['Rect', 'Barker', 'Frank', 'P1', 'P2', 'P3', 'P4', 'Px', 'ZadoffChu', 'LFM', 'FMCW']\n",
    "    image_dir = 'C:/Users/lenovo/OneDrive/Desktop/IP/RadDet-1T-128/RadDet40k128HW001Tv2/images/test'\n",
    "    label_dir = 'C:/Users/lenovo/OneDrive/Desktop/IP/RadDet-1T-128/RadDet40k128HW001Tv2/labels/test'\n",
    "\n",
    "    # image_dir = 'C:/Users/lenovo/OneDrive/Desktop/IP/RadDet-9T-128/RadDet40k128HW009Tv2/images/test'\n",
    "    # label_dir = 'C:/Users/lenovo/OneDrive/Desktop/IP/RadDet-9T-128/RadDet40k128HW009Tv2/labels/test'\n",
    "\n",
    "    # image_dir = 'C:/Users/lenovo/OneDrive/Desktop/IP/NIST-128/NISTSpecMaxHold128Data/images/test'\n",
    "    # label_dir = 'C:/Users/lenovo/OneDrive/Desktop/IP/NIST-128/NISTSpecMaxHold128Data/labels/test'\n",
    "\n",
    "    dataset = RadarDataset(image_dir=image_dir, label_dir=label_dir)\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = YOLOv1().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = YoloLoss()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        loop = tqdm(loader, leave=False)\n",
    "\n",
    "        for imgs, labels in loop:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_description(f\"Epoch {epoch+1}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1} avg loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "    evaluate(model, loader, device, class_names, image_dir, label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c010fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss: 50.8058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 626/626 [06:39<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Inference Speed: 24.84 FPS\n",
      "⚠️ No predictions above threshold. Skipping mAP evaluation.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
