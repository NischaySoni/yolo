{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290be4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bcc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 20 # Increased for better learning\n",
    "NUM_CLASSES = 1\n",
    "S = IMG_SIZE // 16 # Grid size for YOLO (128/16 = 8)\n",
    "# Scaled anchors for 128x128 input. Format: (width, height)\n",
    "# These are relative to the image size.\n",
    "ANCHORS = [\n",
    "    (0.1, 0.15),\n",
    "    (0.2, 0.3),\n",
    "    (0.4, 0.5),\n",
    "]\n",
    "NUM_ANCHORS = len(ANCHORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_width_height(box1_wh, box2_wh):\n",
    "    \"\"\"\n",
    "    Calculates IoU based on width and height, assuming boxes are centered.\n",
    "    Args:\n",
    "        box1_wh (torch.Tensor): Tensor of shape (N, 2) for N boxes' (width, height).\n",
    "        box2_wh (torch.Tensor): Tensor of shape (M, 2) for M boxes' (width, height).\n",
    "    Returns:\n",
    "        torch.Tensor: IoU of shape (N, M).\n",
    "    \"\"\"\n",
    "    intersection_w = torch.min(box1_wh[:, 0:1], box2_wh[:, 0:1].T)\n",
    "    intersection_h = torch.min(box1_wh[:, 1:2], box2_wh[:, 1:2].T)\n",
    "    intersection = intersection_w * intersection_h\n",
    "    union = (box1_wh[:, 0:1] * box1_wh[:, 1:2]) + (box2_wh[:, 0:1].T * box2_wh[:, 1:2].T) - intersection\n",
    "    return intersection / (union + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea35a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, anchors, S=8, C=1):\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n",
    "        self.label_dir = label_dir\n",
    "        self.S = S\n",
    "        self.C = C\n",
    "        self.anchors = torch.tensor(anchors)\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = os.path.join(self.label_dir, os.path.basename(image_path).replace('.png', '.txt'))\n",
    "        image = Image.open(image_path)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Target tensor: [Grid_S, Grid_S, Num_Anchors, 6 (p_o, x, y, w, h, class)]\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.num_anchors, 6))\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    cls, x, y, w, h = map(float, line.strip().split())\n",
    "                    \n",
    "                    # Find which grid cell this object belongs to\n",
    "                    i, j = int(self.S * y), int(self.S * x)\n",
    "                    \n",
    "                    # Find the best anchor for this bounding box\n",
    "                    box_wh = torch.tensor([w, h])\n",
    "                    ious = iou_width_height(box_wh.unsqueeze(0), self.anchors)\n",
    "                    best_anchor_idx = ious.argmax()\n",
    "                    \n",
    "                    # Check if an object is already assigned to this cell and anchor\n",
    "                    if label_matrix[i, j, best_anchor_idx, 0] == 0:\n",
    "                        # Set objectness score to 1\n",
    "                        label_matrix[i, j, best_anchor_idx, 0] = 1.0\n",
    "                        # Set coordinates relative to the cell\n",
    "                        x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "                        # Set width and height relative to image size\n",
    "                        w_cell, h_cell = w, h\n",
    "                        box_coords = torch.tensor([x_cell, y_cell, w_cell, h_cell])\n",
    "                        label_matrix[i, j, best_anchor_idx, 1:5] = box_coords\n",
    "                        # Set class\n",
    "                        label_matrix[i, j, best_anchor_idx, 5] = int(cls)\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, bias=False, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class YoloV3Tiny(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=1, num_anchors=3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "        self.model = nn.Sequential(\n",
    "            CNNBlock(in_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            CNNBlock(16, 32, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            CNNBlock(32, 64, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            CNNBlock(64, 128, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            CNNBlock(128, 256, kernel_size=3, padding=1),\n",
    "            # No final MaxPool, final grid is 8x8\n",
    "            nn.Conv2d(256, num_anchors * (5 + num_classes), kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        # Reshape the output to [Batch, S, S, Num_Anchors, 5 + Num_Classes]\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        B, S, _, _ = out.shape\n",
    "        out = out.view(B, S, S, self.num_anchors, 5 + self.num_classes)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b515055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss() # Numerically stable\n",
    "        self.lambda_class = 1\n",
    "        self.lambda_noobj = 10\n",
    "        self.lambda_obj = 1\n",
    "        self.lambda_box = 10\n",
    "\n",
    "    def forward(self, preds, targets, anchors):\n",
    "        # Identify which cells have objects and which don't\n",
    "        obj_mask = targets[..., 0] == 1\n",
    "        noobj_mask = targets[..., 0] == 0\n",
    "\n",
    "        # === NO OBJECT LOSS ===\n",
    "        noobj_loss = self.bce(\n",
    "            (preds[..., 0:1][noobj_mask]), (targets[..., 0:1][noobj_mask])\n",
    "        )\n",
    "\n",
    "        # === OBJECT LOSS ===\n",
    "        obj_loss = self.bce(\n",
    "            (preds[..., 0:1][obj_mask]), (targets[..., 0:1][obj_mask])\n",
    "        )\n",
    "        \n",
    "        # === BOX COORDINATE LOSS ===\n",
    "        # Transform predictions\n",
    "        preds[..., 1:3] = torch.sigmoid(preds[..., 1:3]) # x, y to be between [0,1]\n",
    "        target_box = targets[..., 1:5][obj_mask]\n",
    "        pred_box = preds[..., 1:5][obj_mask]\n",
    "        \n",
    "        # Apply mse loss for x,y\n",
    "        box_loss_xy = self.mse(pred_box[..., 0:2], target_box[..., 0:2])\n",
    "        \n",
    "        # Transform w,h and apply mse loss\n",
    "        # We want to predict log(w/anchor_w) and log(h/anchor_h)\n",
    "        # For simplicity here we will do MSE on w,h directly but it's better to use the log-space transform\n",
    "        box_loss_wh = self.mse(\n",
    "            torch.sqrt(pred_box[..., 2:4]), torch.sqrt(target_box[..., 2:4])\n",
    "        )\n",
    "        box_loss = box_loss_xy + box_loss_wh\n",
    "\n",
    "        # === CLASS LOSS ===\n",
    "        class_loss = self.bce(\n",
    "            (preds[..., 5:][obj_mask]), (targets[..., 5:][obj_mask].float())\n",
    "        )\n",
    "\n",
    "        total_loss = (\n",
    "            self.lambda_box * box_loss\n",
    "            + self.lambda_obj * obj_loss\n",
    "            + self.lambda_noobj * noobj_loss\n",
    "            + self.lambda_class * class_loss\n",
    "        )\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(bboxes, iou_threshold, confidence_threshold):\n",
    "    \"\"\"\n",
    "    Performs Non-Maximum Suppression on a list of bounding boxes.\n",
    "    Args:\n",
    "        bboxes (list): List of lists containing bounding box information\n",
    "                       [[class, conf, x, y, w, h], ...]\n",
    "        iou_threshold (float): IoU threshold for suppressing boxes.\n",
    "        confidence_threshold (float): Confidence threshold for filtering boxes.\n",
    "    Returns:\n",
    "        list: Bounding boxes after NMS.\n",
    "    \"\"\"\n",
    "    bboxes = [box for box in bboxes if box[1] > confidence_threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0] or \n",
    "               iou_boxes(torch.tensor(chosen_box[2:]), torch.tensor(box[2:])) < iou_threshold\n",
    "        ]\n",
    "\n",
    "    return bboxes_after_nms\n",
    "\n",
    "def iou_boxes(box1, box2):\n",
    "    \"\"\"Calculates IoU between two boxes [x, y, w, h] format.\"\"\"\n",
    "    box1_x1 = box1[0] - box1[2] / 2\n",
    "    box1_y1 = box1[1] - box1[3] / 2\n",
    "    box1_x2 = box1[0] + box1[2] / 2\n",
    "    box1_y2 = box1[1] + box1[3] / 2\n",
    "    box2_x1 = box2[0] - box2[2] / 2\n",
    "    box2_y1 = box2[1] - box2[3] / 2\n",
    "    box2_x2 = box2[0] + box2[2] / 2\n",
    "    box2_y2 = box2[1] + box2[3] / 2\n",
    "\n",
    "    x1 = max(box1_x1, box2_x1)\n",
    "    y1 = max(box1_y1, box2_y1)\n",
    "    x2 = min(box1_x2, box2_x2)\n",
    "    y2 = min(box1_y2, box2_y2)\n",
    "\n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)\n",
    "    area2 = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)\n",
    "    union = area1 + area2 - intersection + 1e-6\n",
    "\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_bboxes(loader, model, iou_threshold, confidence_threshold, anchors, device=\"cpu\"):\n",
    "    \"\"\"Gets all predictions and ground truths from a data loader.\"\"\"\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(tqdm(loader, desc=\"Getting BBoxes\")):\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        bboxes = [[] for _ in range(batch_size)]\n",
    "        \n",
    "        # Get predictions\n",
    "        for i in range(S):\n",
    "            for j in range(S):\n",
    "                for anchor_idx, anchor in enumerate(anchors):\n",
    "                    obj_conf = torch.sigmoid(predictions[..., i, j, anchor_idx, 0])\n",
    "                    if obj_conf > confidence_threshold:\n",
    "                        box_coords = torch.sigmoid(predictions[..., i, j, anchor_idx, 1:3])\n",
    "                        box_wh = predictions[..., i, j, anchor_idx, 3:5]\n",
    "                        \n",
    "                        x_center = (box_coords[0] + j) / S\n",
    "                        y_center = (box_coords[1] + i) / S\n",
    "                        w = box_wh[0] * anchor[0]\n",
    "                        h = box_wh[1] * anchor[1]\n",
    "\n",
    "                        class_label = torch.argmax(predictions[..., i, j, anchor_idx, 5:]).item()\n",
    "                        bboxes[batch_idx].append([class_label, obj_conf.item(), x_center.item(), y_center.item(), w.item(), h.item()])\n",
    "\n",
    "        # Get true boxes\n",
    "        true_bboxes = [[] for _ in range(batch_size)]\n",
    "        for i in range(S):\n",
    "            for j in range(S):\n",
    "                for anchor_idx in range(len(anchors)):\n",
    "                     if y[batch_idx, i, j, anchor_idx, 0] == 1:\n",
    "                        x_center = (y[batch_idx, i, j, anchor_idx, 1] + j) / S\n",
    "                        y_center = (y[batch_idx, i, j, anchor_idx, 2] + i) / S\n",
    "                        w = y[batch_idx, i, j, anchor_idx, 3]\n",
    "                        h = y[batch_idx, i, j, anchor_idx, 4]\n",
    "                        class_label = y[batch_idx, i, j, anchor_idx, 5].item()\n",
    "                        true_bboxes[batch_idx].append([class_label, 1, x_center.item(), y_center.item(), w, h])\n",
    "        \n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(bboxes[idx], iou_threshold, confidence_threshold)\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "            \n",
    "            for box in true_bboxes[idx]:\n",
    "                all_true_boxes.append([train_idx] + box)\n",
    "            \n",
    "            train_idx += 1\n",
    "            \n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, num_classes=1):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision (mAP).\n",
    "    Args:\n",
    "        pred_boxes (list): [[train_idx, class, conf, x, y, w, h], ...]\n",
    "        true_boxes (list): [[train_idx, class, x, y, w, h], ...] (conf is 1)\n",
    "        iou_threshold (float): Threshold for IoU.\n",
    "        num_classes (int): Number of classes.\n",
    "    Returns:\n",
    "        float: mAP value.\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = [d for d in pred_boxes if d[1] == c]\n",
    "        ground_truths = [gt for gt in true_boxes if gt[1] == c]\n",
    "\n",
    "        amount_bboxes = Counter(gt[0] for gt in ground_truths)\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros(len(detections))\n",
    "        FP = torch.zeros(len(detections))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            best_iou = 0\n",
    "            best_gt_idx = -1\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = iou_boxes(torch.tensor(detection[3:]), torch.tensor(gt[2:]))\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / (len(average_precisions) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, boxes):\n",
    "    \"\"\"Plots predicted bounding boxes on the image.\"\"\"\n",
    "    im = np.array(image.permute(1, 2, 0))\n",
    "    height, width, _ = im.shape\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(im)\n",
    "\n",
    "    for box in boxes:\n",
    "        # box format: [class, conf, x, y, w, h]\n",
    "        x, y, w, h = box[2], box[3], box[4], box[5]\n",
    "        \n",
    "        upper_left_x = (x - w / 2) * width\n",
    "        upper_left_y = (y - h / 2) * height\n",
    "        rect = plt.Rectangle(\n",
    "            (upper_left_x, upper_left_y),\n",
    "            w * width,\n",
    "            h * height,\n",
    "            linewidth=2,\n",
    "            edgecolor=\"red\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(upper_left_x, upper_left_y-5, f\"{box[1]:.2f}\", color='white', backgroundcolor='red')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e982ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, optimizer, criterion, device, scaled_anchors):\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        loop = tqdm(loader, leave=True)\n",
    "        total_loss = 0\n",
    "        for imgs, labels in loop:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, labels, scaled_anchors)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_description(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Average Loss: {total_loss / len(loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43bdff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model, device):\n",
    "    print(\"\\n--- Calculating mAP on dataset ---\")\n",
    "    model.to(device)\n",
    "    pred_boxes, true_boxes = get_all_bboxes(\n",
    "        loader, model, \n",
    "        iou_threshold=0.5, \n",
    "        confidence_threshold=0.5, \n",
    "        anchors=ANCHORS, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    map_val = mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, num_classes=NUM_CLASSES)\n",
    "    print(f\"mAP: {map_val:.4f}\")\n",
    "    return map_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_data(img_dir=\"images\", label_dir=\"labels\", num_samples=50):\n",
    "    \"\"\"Generates fake images and labels for testing.\"\"\"\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    os.makedirs(label_dir, exist_ok=True)\n",
    "    print(\"Generating dummy data...\")\n",
    "    for i in range(num_samples):\n",
    "        # Create a black image with a white rectangle (our \"object\")\n",
    "        img = Image.new('L', (IMG_SIZE, IMG_SIZE), color='black')\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        w = np.random.uniform(0.1, 0.4)\n",
    "        h = np.random.uniform(0.1, 0.4)\n",
    "        x = np.random.uniform(w/2, 1-w/2)\n",
    "        y = np.random.uniform(h/2, 1-h/2)\n",
    "        \n",
    "        x1 = (x - w/2) * IMG_SIZE\n",
    "        y1 = (y - h/2) * IMG_SIZE\n",
    "        x2 = (x + w/2) * IMG_SIZE\n",
    "        y2 = (y + h/2) * IMG_SIZE\n",
    "        draw.rectangle([x1, y1, x2, y2], fill='white')\n",
    "        \n",
    "        img.save(os.path.join(img_dir, f\"img_{i:03d}.png\"))\n",
    "        \n",
    "        with open(os.path.join(label_dir, f\"img_{i:03d}.txt\"), 'w') as f:\n",
    "            f.write(f\"0 {x} {y} {w} {h}\\n\") # Class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a5429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Setup ---\n",
    "    image_dir = \"images\"\n",
    "    label_dir = \"labels\"\n",
    "    generate_dummy_data(image_dir, label_dir) # Use this to create test data\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Scale anchors to grid size\n",
    "    scaled_anchors = (\n",
    "        torch.tensor(ANCHORS) * torch.tensor([S, S]).unsqueeze(1).T\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Data Loading ---\n",
    "    dataset = RadarDataset(image_dir, label_dir, anchors=ANCHORS, S=S, C=NUM_CLASSES)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # --- Model, Optimizer, Loss ---\n",
    "    model = YoloV3Tiny(num_classes=NUM_CLASSES, num_anchors=NUM_ANCHORS).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = YoloLoss()\n",
    "    \n",
    "    # --- Training ---\n",
    "    train_model(model, loader, optimizer, criterion, device, scaled_anchors)\n",
    "    \n",
    "    # --- Evaluation & Visualization ---\n",
    "    check_accuracy(loader, model, device)\n",
    "\n",
    "    # Visualize a prediction\n",
    "    model.eval()\n",
    "    x, y = next(iter(loader))\n",
    "    x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "    \n",
    "    # Get bboxes for the first image in the batch\n",
    "    bboxes = []\n",
    "    for i in range(S):\n",
    "        for j in range(S):\n",
    "            for anchor_idx in range(NUM_ANCHORS):\n",
    "                conf = torch.sigmoid(out[0, i, j, anchor_idx, 0])\n",
    "                if conf > 0.5:\n",
    "                     box_coords = torch.sigmoid(out[0, i, j, anchor_idx, 1:3])\n",
    "                     box_wh = out[0, i, j, anchor_idx, 3:5] # These are log-space, need exp transform\n",
    "                     x_center = (box_coords[0] + j) / S\n",
    "                     y_center = (box_coords[1] + i) / S\n",
    "                     # Note: for plotting, we need to apply the inverse transform for w,h\n",
    "                     # This depends on how loss is calculated. Since we did sqrt in loss\n",
    "                     # we can just use the direct relative w,h from target for viz\n",
    "                     w = y[0, i, j, anchor_idx, 3] # cheating a bit for viz\n",
    "                     h = y[0, i, j, anchor_idx, 4] # cheating a bit for viz\n",
    "                     bboxes.append([0, conf.item(), x_center.item(), y_center.item(), w.item(), h.item()])\n",
    "    \n",
    "    nms_boxes = non_max_suppression(bboxes, iou_threshold=0.5, confidence_threshold=0.5)\n",
    "    print(\"\\nVisualizing a sample prediction:\")\n",
    "    plot_image(x[0].cpu(), nms_boxes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
