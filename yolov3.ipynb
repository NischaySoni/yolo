{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290be4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bcc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 416\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-5 # Lower LR for a more complex model\n",
    "EPOCHS = 10 \n",
    "NUM_CLASSES = 11\n",
    "CONFIDENCE_THRESHOLD = 0.6\n",
    "IOU_THRESHOLD = 0.5\n",
    "ANCHORS = [\n",
    "    [(116, 90), (156, 198), (373, 326)],  # Scale 1 (13x13) for large objects\n",
    "    [(30, 61), (62, 45), (59, 119)],      # Scale 2 (26x26) for medium objects\n",
    "    [(10, 13), (16, 30), (33, 23)],        # Scale 3 (52x52) for small objects\n",
    "]\n",
    "S = [IMG_SIZE // 32, IMG_SIZE // 16, IMG_SIZE // 8] # Strides, Grid sizes: [13, 26, 52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_bn=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=not use_bn, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leaky(self.bn(self.conv(x)))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
    "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            ]\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        return x\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, anchors):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.anchors = anchors\n",
    "        self.num_anchors = len(anchors)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            CNNBlock(in_channels, in_channels * 2, kernel_size=3, padding=1),\n",
    "            CNNBlock(in_channels * 2, (self.num_anchors * (5 + num_classes)), use_bn=False, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape the output to [Batch, Num_Anchors, Grid_S, Grid_S, 5 + Num_Classes]\n",
    "        out = self.head(x)\n",
    "        out = out.view(x.shape[0], self.num_anchors, 5 + self.num_classes, x.shape[2], x.shape[3])\n",
    "        out = out.permute(0, 1, 3, 4, 2)\n",
    "        return out\n",
    "\n",
    "class Darknet53(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = nn.ModuleList([\n",
    "            CNNBlock(in_channels, 32, kernel_size=3, padding=1),\n",
    "            CNNBlock(32, 64, kernel_size=3, padding=1, stride=2),\n",
    "            ResidualBlock(64, num_repeats=1),\n",
    "            CNNBlock(64, 128, kernel_size=3, padding=1, stride=2),\n",
    "            ResidualBlock(128, num_repeats=2),\n",
    "            CNNBlock(128, 256, kernel_size=3, padding=1, stride=2),\n",
    "            ResidualBlock(256, num_repeats=8), # -> Route 1\n",
    "            CNNBlock(256, 512, kernel_size=3, padding=1, stride=2),\n",
    "            ResidualBlock(512, num_repeats=8), # -> Route 2\n",
    "            CNNBlock(512, 1024, kernel_size=3, padding=1, stride=2),\n",
    "            ResidualBlock(1024, num_repeats=4),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            # The routes are the outputs of the last three residual blocks\n",
    "            if i in [6, 8]:\n",
    "                outputs.append(x)\n",
    "        outputs.append(x)\n",
    "        return outputs[0], outputs[1], outputs[2] # 52x52, 26x26, 13x13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf0c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.backbone = Darknet53(in_channels=in_channels)\n",
    "        \n",
    "        # Prediction Heads for each scale\n",
    "        self.head1 = PredictionHead(1024, num_classes, ANCHORS[0]) # Large scale\n",
    "        self.head2 = PredictionHead(512, num_classes, ANCHORS[1])  # Medium scale\n",
    "        self.head3 = PredictionHead(256, num_classes, ANCHORS[2])  # Small scale\n",
    "\n",
    "        self.conv1 = CNNBlock(1024, 512, kernel_size=1)\n",
    "        self.conv2 = CNNBlock(512, 256, kernel_size=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        route3, route2, route1 = self.backbone(x) # small, medium, large routes\n",
    "        \n",
    "        # Scale 1 prediction (large objects)\n",
    "        out1 = self.head1(route1)\n",
    "        \n",
    "        # Scale 2 prediction (medium objects)\n",
    "        x = self.conv1(route1)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, route2], dim=1)\n",
    "        out2 = self.head2(x)\n",
    "\n",
    "        # Scale 3 prediction (small objects)\n",
    "        x = self.conv2(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, route3], dim=1)\n",
    "        out3 = self.head3(x)\n",
    "\n",
    "        return out1, out2, out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe166a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_width_height(box1_wh, box2_wh):\n",
    "    \"\"\"\n",
    "    Calculates IoU based on width and height, assuming boxes are centered.\n",
    "    This is used for matching ground truth boxes to the best anchor box.\n",
    "    \n",
    "    Args:\n",
    "        box1_wh (torch.Tensor): Tensor of shape (N, 2) for N boxes' (width, height).\n",
    "        box2_wh (torch.Tensor): Tensor of shape (M, 2) for M boxes' (width, height).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: IoU of shape (N, M).\n",
    "    \"\"\"\n",
    "    intersection_w = torch.min(box1_wh[:, 0:1], box2_wh[:, 0:1].T)\n",
    "    intersection_h = torch.min(box1_wh[:, 1:2], box2_wh[:, 1:2].T)\n",
    "    intersection = intersection_w * intersection_h\n",
    "    \n",
    "    box1_area = box1_wh[:, 0:1] * box1_wh[:, 1:2]\n",
    "    box2_area = box2_wh[:, 0:1].T * box2_wh[:, 1:2].T\n",
    "    union = box1_area + box2_area - intersection\n",
    "    \n",
    "    return intersection / (union + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c44aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_boxes(box1, box2, box_format=\"xywh\"):\n",
    "    \"\"\"\n",
    "    Calculates Intersection over Union (IoU) between two bounding boxes.\n",
    "    This is used during evaluation (NMS and mAP).\n",
    "\n",
    "    Args:\n",
    "        box1 (torch.Tensor): Bounding box 1.\n",
    "        box2 (torch.Tensor): Bounding box 2.\n",
    "        box_format (str): \"xywh\" (center_x, center_y, width, height) or \"xyxy\" (x1, y1, x2, y2).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: IoU value.\n",
    "    \"\"\"\n",
    "    if box_format == \"xywh\":\n",
    "        box1_x1 = box1[..., 0:1] - box1[..., 2:3] / 2\n",
    "        box1_y1 = box1[..., 1:2] - box1[..., 3:4] / 2\n",
    "        box1_x2 = box1[..., 0:1] + box1[..., 2:3] / 2\n",
    "        box1_y2 = box1[..., 1:2] + box1[..., 3:4] / 2\n",
    "        box2_x1 = box2[..., 0:1] - box2[..., 2:3] / 2\n",
    "        box2_y1 = box2[..., 1:2] - box2[..., 3:4] / 2\n",
    "        box2_x2 = box2[..., 0:1] + box2[..., 2:3] / 2\n",
    "        box2_y2 = box2[..., 1:2] + box2[..., 3:4] / 2\n",
    "    elif box_format == \"xyxy\":\n",
    "        box1_x1, box1_y1, box1_x2, box1_y2 = box1[..., 0:1], box1[..., 1:2], box1[..., 2:3], box1[..., 3:4]\n",
    "        box2_x1, box2_y1, box2_x2, box2_y2 = box2[..., 0:1], box2[..., 1:2], box2[..., 2:3], box2[..., 3:4]\n",
    "\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    area1 = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    area2 = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    union = area1 + area2 - intersection + 1e-6\n",
    "\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a154fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(bboxes, iou_threshold, confidence_threshold):\n",
    "    \"\"\"\n",
    "    Performs Non-Maximum Suppression on a list of bounding boxes to filter duplicates.\n",
    "    \n",
    "    Args:\n",
    "        bboxes (list): List of lists: [[class, conf, x, y, w, h], ...]\n",
    "        iou_threshold (float): IoU threshold for suppressing boxes.\n",
    "        confidence_threshold (float): Confidence threshold for filtering boxes.\n",
    "    \n",
    "    Returns:\n",
    "        list: Bounding boxes after NMS.\n",
    "    \"\"\"\n",
    "    # Filter out boxes with low confidence\n",
    "    bboxes = [box for box in bboxes if box[1] > confidence_threshold]\n",
    "    # Sort boxes by confidence score in descending order\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "        \n",
    "        # Keep only boxes of different classes or with low IoU\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0] or \n",
    "               iou_boxes(torch.tensor(chosen_box[2:]), torch.tensor(box[2:])) < iou_threshold\n",
    "        ]\n",
    "        \n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d1794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, anchors, S, C=1):\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n",
    "        self.label_dir = label_dir\n",
    "        self.S = S\n",
    "        self.C = C\n",
    "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "        self.ignore_iou_thresh = 0.5\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = os.path.join(self.label_dir, os.path.basename(image_path).replace('.png', '.txt'))\n",
    "        image = Image.open(image_path)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        targets = [torch.zeros((self.num_anchors_per_scale, s, s, 6)) for s in self.S]\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    cls, x, y, w, h = map(float, line.strip().split())\n",
    "                    \n",
    "                    # Find the best anchor for this bounding box across ALL anchors\n",
    "                    ious = iou_width_height(torch.tensor([w, h]), self.anchors)\n",
    "                    best_anchor_idx = ious.argmax()\n",
    "                    \n",
    "                    # Determine which scale and which anchor on that scale it belongs to\n",
    "                    scale_idx = best_anchor_idx // self.num_anchors_per_scale\n",
    "                    anchor_on_scale_idx = best_anchor_idx % self.num_anchors_per_scale\n",
    "                    \n",
    "                    s = self.S[scale_idx]\n",
    "                    i, j = int(s * y), int(s * x) # grid cell\n",
    "                    \n",
    "                    # Check if cell is already taken\n",
    "                    if targets[scale_idx][anchor_on_scale_idx, i, j, 0] == 0:\n",
    "                        targets[scale_idx][anchor_on_scale_idx, i, j, 0] = 1 \n",
    "                        x_cell, y_cell = s * x - j, s * y - i\n",
    "                        w_cell, h_cell = w, h\n",
    "                        box_coords = torch.tensor([x_cell, y_cell, w_cell, h_cell])\n",
    "                        targets[scale_idx][anchor_on_scale_idx, i, j, 1:5] = box_coords\n",
    "                        targets[scale_idx][anchor_on_scale_idx, i, j, 5] = int(cls)\n",
    "\n",
    "        return image, tuple(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd43406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.lambda_class = 1\n",
    "        self.lambda_noobj = 10\n",
    "        self.lambda_obj = 1\n",
    "        self.lambda_box = 10\n",
    "\n",
    "    def forward(self, predictions, targets, anchors):\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Iterate over the 3 scales\n",
    "        for i in range(3):\n",
    "            pred = predictions[i]\n",
    "            target = targets[i]\n",
    "            # Anchors for the current scale\n",
    "            scale_anchors = anchors[i]\n",
    "            \n",
    "            obj_mask = target[..., 0] == 1\n",
    "            noobj_mask = target[..., 0] == 0\n",
    "\n",
    "            # No Object Loss\n",
    "            noobj_loss = self.bce(\n",
    "                (pred[..., 0:1][noobj_mask]), (target[..., 0:1][noobj_mask])\n",
    "            )\n",
    "\n",
    "            # Object Loss\n",
    "            obj_loss = self.bce(\n",
    "                (pred[..., 0:1][obj_mask]), (target[..., 0:1][obj_mask])\n",
    "            )\n",
    "\n",
    "            # Box Coordinate Loss\n",
    "            pred[..., 1:3] = torch.sigmoid(pred[..., 1:3]) # x,y\n",
    "            target[..., 3:5] = torch.log(\n",
    "                (1e-16 + target[..., 3:5] / scale_anchors)\n",
    "            ) # w,h\n",
    "            box_loss = self.mse(pred[..., 1:5][obj_mask], target[..., 1:5][obj_mask])\n",
    "            \n",
    "            # Class Loss\n",
    "            class_loss = self.bce(\n",
    "                (pred[..., 5:][obj_mask]), (target[..., 5:][obj_mask].float())\n",
    "            )\n",
    "            \n",
    "            total_loss += (\n",
    "                self.lambda_box * box_loss\n",
    "                + self.lambda_obj * obj_loss\n",
    "                + self.lambda_noobj * noobj_loss\n",
    "                + self.lambda_class * class_loss\n",
    "            )\n",
    "            \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_bboxes(loader, model, iou_threshold, confidence_threshold, anchors, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Gets all predictions and ground truths from a data loader for the multi-scale model.\n",
    "    \n",
    "    Args:\n",
    "        loader: The DataLoader for the dataset.\n",
    "        model: The trained YOLOv3 model.\n",
    "        iou_threshold (float): IoU threshold for NMS.\n",
    "        confidence_threshold (float): Confidence threshold for filtering predictions.\n",
    "        anchors (list): The list of anchor boxes.\n",
    "        device (str): The device to run on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "               - all_pred_boxes: [[train_idx, class, conf, x, y, w, h], ...]\n",
    "               - all_true_boxes: [[train_idx, class, 1, x, y, w, h], ...]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    scaled_anchors = (\n",
    "        torch.tensor(anchors)\n",
    "        .reshape((3, 3, 2))\n",
    "        .to(device)\n",
    "    )\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(tqdm(loader, desc=\"Getting BBoxes\")):\n",
    "        x = x.to(device)\n",
    "        \n",
    "        y = (y[0].to(device), y[1].to(device), y[2].to(device))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            pred_boxes_single_image = []\n",
    "            # For each scale\n",
    "            for scale_idx in range(3):\n",
    "                S = predictions[scale_idx].shape[2]\n",
    "                # For each anchor on that scale\n",
    "                for anchor_idx in range(3):\n",
    "                    # Get all predictions where objectness is above threshold\n",
    "                    obj_conf = torch.sigmoid(predictions[scale_idx][i, anchor_idx, ..., 0])\n",
    "                    conf_mask = obj_conf > confidence_threshold\n",
    "                    \n",
    "                    if not conf_mask.any():\n",
    "                        continue\n",
    "\n",
    "                    # Extract confident predictions\n",
    "                    preds_on_scale = predictions[scale_idx][i, anchor_idx, conf_mask, :]\n",
    "                    grid_y, grid_x = torch.where(conf_mask)\n",
    "                    \n",
    "                    # Decode bounding box coordinates\n",
    "                    box_coords = torch.sigmoid(preds_on_scale[:, 1:3])\n",
    "                    x_center = (box_coords[:, 0] + grid_x) / S\n",
    "                    y_center = (box_coords[:, 1] + grid_y) / S\n",
    "                    \n",
    "                    # Decode width and height\n",
    "                    anchor = scaled_anchors[scale_idx, anchor_idx]\n",
    "                    box_wh = torch.exp(preds_on_scale[:, 3:5]) * anchor\n",
    "                    w = box_wh[:, 0] / IMG_SIZE\n",
    "                    h = box_wh[:, 1] / IMG_SIZE\n",
    "                    \n",
    "                    # Get class predictions\n",
    "                    class_probs = torch.sigmoid(preds_on_scale[:, 5:])\n",
    "                    class_conf, class_label = torch.max(class_probs, dim=1)\n",
    "                    \n",
    "                    # Combine into [class, conf, x, y, w, h] format\n",
    "                    final_conf = (torch.sigmoid(preds_on_scale[:, 0]) * class_conf).float()\n",
    "                    \n",
    "                    # Filter again by the final confidence\n",
    "                    final_conf_mask = final_conf > confidence_threshold\n",
    "                    if not final_conf_mask.any():\n",
    "                        continue\n",
    "                        \n",
    "                    pred_boxes_batch = torch.cat([\n",
    "                        class_label[final_conf_mask].float().unsqueeze(1),\n",
    "                        final_conf[final_conf_mask].unsqueeze(1),\n",
    "                        x_center[final_conf_mask].unsqueeze(1),\n",
    "                        y_center[final_conf_mask].unsqueeze(1),\n",
    "                        w[final_conf_mask].unsqueeze(1),\n",
    "                        h[final_conf_mask].unsqueeze(1)\n",
    "                    ], dim=1)\n",
    "                    \n",
    "                    pred_boxes_single_image.extend(pred_boxes_batch.tolist())\n",
    "\n",
    "            # Run NMS on all boxes for this image\n",
    "            nms_boxes = non_max_suppression(pred_boxes_single_image, iou_threshold, confidence_threshold)\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            # Extract ground truth boxes for this image\n",
    "            for scale_idx in range(3):\n",
    "                S = y[scale_idx].shape[2]\n",
    "                for anchor_idx in range(3):\n",
    "                    obj_mask = y[scale_idx][i, anchor_idx, ..., 0] == 1\n",
    "                    if not obj_mask.any():\n",
    "                        continue\n",
    "                        \n",
    "                    true_boxes_on_scale = y[scale_idx][i, anchor_idx, obj_mask, :]\n",
    "                    grid_y, grid_x = torch.where(obj_mask)\n",
    "                    \n",
    "                    x_center = (true_boxes_on_scale[:, 1] + grid_x) / S\n",
    "                    y_center = (true_boxes_on_scale[:, 2] + grid_y) / S\n",
    "                    w = true_boxes_on_scale[:, 3]\n",
    "                    h = true_boxes_on_scale[:, 4]\n",
    "                    class_label = true_boxes_on_scale[:, 5]\n",
    "                    \n",
    "                    true_boxes_batch = torch.cat([\n",
    "                        class_label.unsqueeze(1),\n",
    "                        torch.ones_like(class_label).unsqueeze(1), # Confidence is 1 for true boxes\n",
    "                        x_center.unsqueeze(1),\n",
    "                        y_center.unsqueeze(1),\n",
    "                        w.unsqueeze(1),\n",
    "                        h.unsqueeze(1),\n",
    "                    ], dim=1)\n",
    "                    \n",
    "                    all_true_boxes.extend([[train_idx] + box for box in true_boxes_batch.tolist()])\n",
    "            \n",
    "            train_idx += 1\n",
    "            \n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50852d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, optimizer, criterion, device, scaled_anchors):\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        loop = tqdm(loader, leave=True)\n",
    "        total_loss = 0\n",
    "        for imgs, labels in loop:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = (\n",
    "                labels[0].to(device),\n",
    "                labels[1].to(device),\n",
    "                labels[2].to(device),\n",
    "            )\n",
    "            \n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, labels, scaled_anchors)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_description(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Average Loss: {total_loss / len(loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, num_classes=1):\n",
    "    \"\"\"\n",
    "    Calculates mean Average Precision (mAP), the standard metric for object detection.\n",
    "    \n",
    "    Args:\n",
    "        pred_boxes (list): [[train_idx, class, conf, x, y, w, h], ...]\n",
    "        true_boxes (list): [[train_idx, class, conf, x, y, w, h], ...]\n",
    "        iou_threshold (float): Threshold for a detection to be a True Positive.\n",
    "        num_classes (int): Number of classes in the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        float: mAP value across all classes.\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = [d for d in pred_boxes if d[1] == c]\n",
    "        ground_truths = [gt for gt in true_boxes if gt[1] == c]\n",
    "\n",
    "        # Count how many ground truth boxes are in each image\n",
    "        amount_bboxes = Counter(gt[0] for gt in ground_truths)\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # Sort detections by confidence\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros(len(detections))\n",
    "        FP = torch.zeros(len(detections))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Get all ground truth boxes for the same image as the detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            best_iou = 0\n",
    "            best_gt_idx = -1\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = iou_boxes(torch.tensor(detection[3:]), torch.tensor(gt[3:]))\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # Check if we haven't already matched this ground truth box\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    TP[detection_idx] = 1 # Mark as True Positive\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1 # It's a duplicate detection\n",
    "            else:\n",
    "                FP[detection_idx] = 1 # Failed to meet IoU threshold\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "        \n",
    "        # Integrate under the precision-recall curve\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / (len(average_precisions) + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a0bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model, device):\n",
    "    \"\"\"\n",
    "    Runs the full evaluation pipeline: gets bounding boxes, calculates mAP.\n",
    "    \"\"\"\n",
    "    print(\"\\nCalculating mAP on dataset \")\n",
    "    model.to(device)\n",
    "    pred_boxes, true_boxes = get_all_bboxes(\n",
    "        loader, model, \n",
    "        iou_threshold=IOU_THRESHOLD, \n",
    "        confidence_threshold=CONFIDENCE_THRESHOLD, \n",
    "        anchors=ANCHORS, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    map_val = mean_average_precision(pred_boxes, true_boxes, iou_threshold=IOU_THRESHOLD, num_classes=NUM_CLASSES)\n",
    "    print(f\"mAP: {map_val:.4f}\")\n",
    "    return map_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image_tensor, boxes):\n",
    "    \"\"\"\n",
    "    Plots predicted bounding boxes on a single image.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): A single image tensor of shape [C, H, W].\n",
    "        boxes (list): A list of bounding boxes for that image.\n",
    "    \"\"\"\n",
    "    # Convert tensor to PIL Image\n",
    "    im = transforms.ToPILImage()(image_tensor.cpu())\n",
    "    if im.mode != \"RGB\":\n",
    "        im = im.convert(\"RGB\")\n",
    "        \n",
    "    draw = ImageDraw.Draw(im)\n",
    "    width, height = im.size\n",
    "\n",
    "    for box in boxes:\n",
    "        # box format: [class, conf, x, y, w, h]\n",
    "        class_pred, conf, x, y, w, h = box\n",
    "        \n",
    "        # Convert from center format to top-left corner format\n",
    "        upper_left_x = (x - w / 2) * width\n",
    "        upper_left_y = (y - h / 2) * height\n",
    "        lower_right_x = (x + w / 2) * width\n",
    "        lower_right_y = (y + h / 2) * height\n",
    "\n",
    "        # Draw bounding box\n",
    "        draw.rectangle(\n",
    "            [upper_left_x, upper_left_y, lower_right_x, lower_right_y],\n",
    "            outline=\"red\",\n",
    "            width=2\n",
    "        )\n",
    "        \n",
    "        # Draw label\n",
    "        text = f\"Class {int(class_pred)}: {conf:.2f}\"\n",
    "        text_bbox = draw.textbbox((upper_left_x, upper_left_y), text)\n",
    "        draw.rectangle(text_bbox, fill=\"red\")\n",
    "        draw.text((upper_left_x, upper_left_y), text, fill=\"white\")\n",
    "\n",
    "    plt.imshow(im)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc042d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    image_dir = \"C:/Users/lenevo/OneDrive/Desktop/IP/RadDet-1T-128/RadDet40k128HW001Tv2/images\"\n",
    "    label_dir = \"C:/Users/lenevo/OneDrive/Desktop/IP/RadDet-1T-128/RadDet40k128HW001Tv2/labels\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Anchors for loss calculation \n",
    "    scaled_anchors = (\n",
    "        torch.tensor(ANCHORS) / torch.tensor([IMG_SIZE, IMG_SIZE]).view(1, 1, 2)\n",
    "    ).to(device)\n",
    "\n",
    "    dataset = RadarDataset(image_dir, label_dir, anchors=ANCHORS, S=S, C=NUM_CLASSES)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    model = YOLOv3(in_channels=1, num_classes=NUM_CLASSES).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    criterion = YoloLoss()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_model(model, loader, optimizer, criterion, device, scaled_anchors)\n",
    "        check_accuracy(loader, model, device)\n",
    "    \n",
    "    print(\"\\nFinal Evaluation: \")\n",
    "    check_accuracy(loader, model, device)\n",
    "    \n",
    "    print(\"\\nVisualizing a sample prediction: \")\n",
    "    model.eval()\n",
    "    x, y = next(iter(loader))\n",
    "    x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "    \n",
    "    all_preds, _ = get_all_bboxes(\n",
    "        [(x, y)], model, IOU_THRESHOLD, CONFIDENCE_THRESHOLD, ANCHORS, device\n",
    "    )\n",
    "    \n",
    "    # Filter boxes for the first image in the batch\n",
    "    boxes_for_image_0 = [box[1:] for box in all_preds if box[0] == 0]\n",
    "    \n",
    "    plot_image(x[0], boxes_for_image_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
